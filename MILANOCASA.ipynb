{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTEBOOK Voilà-friendly (Binder)\n",
        "# Scarica <p> dopo il divider successivo all'H2 \"Descrizione\" + immagini galleria da milanocasa.com\n",
        "\n",
        "import os, re, io, time, json, shutil, base64\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# ====== Parametri base ======\n",
        "BASE_LIST_URL = \"https://www.milanocasa.com/immobili\"\n",
        "DELAY_SEC = 1\n",
        "USER_AGENT = \"Mozilla/5.0 (compatible; milanocasa-scraper/1.0; +https://example.com)\"\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"it-IT,it;q=0.9\"})\n",
        "\n",
        "# ---- Funzioni varie scraping ----\n",
        "def slugify(text, maxlen=80):\n",
        "    text = re.sub(r\"\\s+\", \"-\", text.strip())\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\-_]+\", \"\", text)\n",
        "    return (text[:maxlen] or \"item\").strip(\"-_\")\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def fetch_soup(url):\n",
        "    r = session.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "def is_property_link(href: str, base_domain: str) -> bool:\n",
        "    if not href:\n",
        "        return False\n",
        "    p = urlparse(href)\n",
        "    domain_ok = (p.netloc == \"\" or p.netloc.endswith(base_domain))\n",
        "    bad_paths = (\"/immobili\", \"/category/\", \"/categoria/\", \"/tag/\")\n",
        "    looks_like_detail = re.search(r\"/immobil|/property|/scheda|/vendita|/affitto\", p.path or \"\", re.I)\n",
        "    return domain_ok and not any((p.path or \"\").startswith(b) for b in bad_paths) and bool(looks_like_detail)\n",
        "\n",
        "def extract_last_n_property_links(list_url: str, n=10):\n",
        "    base_domain = urlparse(list_url).netloc or \"milanocasa.com\"\n",
        "    soup = fetch_soup(list_url)\n",
        "    links, seen = [], set()\n",
        "    for a in soup.select(\"a[href]\"):\n",
        "        href_abs = urljoin(list_url, a.get(\"href\") or \"\")\n",
        "        if is_property_link(href_abs, base_domain) and href_abs not in seen:\n",
        "            seen.add(href_abs); links.append(href_abs)\n",
        "        if len(links) >= n: break\n",
        "    return links\n",
        "\n",
        "def extract_title(soup: BeautifulSoup):\n",
        "    h1 = soup.select_one(\"h1\")\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        return h1.get_text(strip=True)\n",
        "    if soup.title and soup.title.string:\n",
        "        return soup.title.string.strip()\n",
        "    return \"Immobile\"\n",
        "\n",
        "def extract_gallery_images(soup: BeautifulSoup, base_for_urls: str):\n",
        "    urls, out, seen = [], [], set()\n",
        "    for gw in soup.select(\".elementor-widget-pp-image-gallery\"):\n",
        "        for img in gw.select(\"img\"):\n",
        "            cand = img.get(\"data-src\") or img.get(\"data-lazy-src\") or img.get(\"src\")\n",
        "            if not cand and img.get(\"srcset\"):\n",
        "                cand = img.get(\"srcset\").split(\",\")[0].strip().split(\" \")[0]\n",
        "            if cand:\n",
        "                urls.append(cand)\n",
        "        for a in gw.select(\"a[href]\"):\n",
        "            href = a[\"href\"]\n",
        "            if re.search(r\"\\.(jpe?g|png|webp|gif)(\\?.*)?$\", href, re.I):\n",
        "                urls.append(href)\n",
        "        for tag in gw.find_all(True):\n",
        "            for _, val in list(tag.attrs.items()):\n",
        "                if isinstance(val, str) and re.search(r\"\\.(jpe?g|png|webp|gif)(\\?.*)?$\", val, re.I):\n",
        "                    urls.append(val)\n",
        "    for u in urls:\n",
        "        u_abs = urljoin(base_for_urls, u)\n",
        "        if u_abs not in seen:\n",
        "            seen.add(u_abs); out.append(u_abs)\n",
        "    return out\n",
        "\n",
        "def is_divider_like(tag):\n",
        "    if not getattr(tag, \"name\", None):\n",
        "        return False\n",
        "    if tag.name.lower() == \"hr\":\n",
        "        return True\n",
        "    classes = \" \".join(tag.get(\"class\", []))\n",
        "    return any(k in classes for k in [\"elementor-divider\", \"divider\", \"elementor-divider__separator\"])\n",
        "\n",
        "def extract_description(soup: BeautifulSoup):\n",
        "    for hdr in soup.find_all([\"h2\", \"h3\"]):\n",
        "        hdr_text = hdr.get_text(\" \", strip=True).lower()\n",
        "        if \"descrizione\" in hdr_text:\n",
        "            divider = None\n",
        "            for tag in hdr.find_all_next():\n",
        "                if tag is hdr:\n",
        "                    continue\n",
        "                if getattr(tag, \"name\", \"\").lower() in {\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"}:\n",
        "                    break\n",
        "                if is_divider_like(tag):\n",
        "                    divider = tag\n",
        "                    break\n",
        "            if divider:\n",
        "                for tag in divider.find_all_next():\n",
        "                    if getattr(tag, \"name\", \"\").lower() in {\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"}:\n",
        "                        break\n",
        "                    if getattr(tag, \"name\", \"\").lower() == \"p\":\n",
        "                        text = tag.get_text(separator=\"\\n\", strip=True)\n",
        "                        if text:\n",
        "                            return re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
        "            for tag in hdr.find_all_next():\n",
        "                if getattr(tag, \"name\", \"\").lower() in {\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"}:\n",
        "                    break\n",
        "                if getattr(tag, \"name\", \"\").lower() == \"p\":\n",
        "                    text = tag.get_text(separator=\"\\n\", strip=True)\n",
        "                    if text:\n",
        "                        return re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
        "            return \"\"\n",
        "    return \"\"\n",
        "\n",
        "def download_file(url, dest_path, retries=2):\n",
        "    for attempt in range(retries+1):\n",
        "        try:\n",
        "            with session.get(url, stream=True, timeout=60) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(dest_path, \"wb\") as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "            return True\n",
        "        except Exception:\n",
        "            if attempt == retries:\n",
        "                return False\n",
        "            time.sleep(1)\n",
        "\n",
        "# ====== Download controls robusti ======\n",
        "download_area = widgets.Output()\n",
        "\n",
        "def _voila_prefix():\n",
        "    root = os.environ.get(\"JUPYTERHUB_SERVICE_PREFIX\", \"/\")\n",
        "    if not root.endswith(\"/\"):\n",
        "        root += \"/\"\n",
        "    return root\n",
        "\n",
        "def _show_download_controls(zip_path: str):\n",
        "    with download_area:\n",
        "        download_area.clear_output()\n",
        "        name = os.path.basename(zip_path)\n",
        "        size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
        "        print(f\"[INFO] ZIP: {name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "        if hasattr(widgets, \"FileDownload\"):\n",
        "            def _zip_bytes():\n",
        "                with open(zip_path, \"rb\") as f:\n",
        "                    return f.read()\n",
        "            btn = widgets.FileDownload(\n",
        "                data=_zip_bytes,\n",
        "                filename=name,\n",
        "                description=\"⬇️ Scarica ZIP\",\n",
        "                button_style=\"primary\",\n",
        "                icon=\"download\"\n",
        "            )\n",
        "            display(btn)\n",
        "        else:\n",
        "            print(\"[INFO] ipywidgets<8: uso fallback senza FileDownload\")\n",
        "\n",
        "        with open(zip_path, \"rb\") as f:\n",
        "            b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
        "        html_data = (\n",
        "            f'<p><a download=\"{name}\" href=\"data:application/zip;base64,{b64}\">'\n",
        "            f'⬇️ Scarica ZIP</a></p>'\n",
        "        )\n",
        "        display(HTML(html_data))\n",
        "\n",
        "        pref = _voila_prefix()\n",
        "        abs_href = f\"{pref}voila/files/{name}\"\n",
        "        rel_href = f\"../files/{name}\"\n",
        "        html_extra = (\n",
        "            f'<p>Altri link (se il data-URI avesse problemi): '\n",
        "            f'<a href=\"{abs_href}\" target=\"_blank\">{abs_href}</a> | '\n",
        "            f'<a href=\"{rel_href}\" target=\"_blank\">{rel_href}</a></p>'\n",
        "        )\n",
        "        # display(HTML(html_extra))\n",
        "\n",
        "# ====== Widget interfaccia ======\n",
        "n_items_widget = widgets.BoundedIntText(value=5, min=1, max=50, step=1, description=\"Annunci:\")\n",
        "btn_start = widgets.Button(description=\"Avvia scraping\", button_style=\"success\")\n",
        "out = widgets.Output()\n",
        "\n",
        "def run_scraper(b):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        N_ITEMS = n_items_widget.value\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        root_dir = ensure_dir(f\"./milanocasa_export_{timestamp}\")\n",
        "        manifest = []\n",
        "\n",
        "        links = extract_last_n_property_links(BASE_LIST_URL, N_ITEMS)\n",
        "        print(f\"Trovati {len(links)} link. Avvio download...\\n\")\n",
        "\n",
        "        for idx, url in enumerate(links, 1):\n",
        "            print(f\"[{idx}/{len(links)}] {url}\")\n",
        "            item = {\"url\": url, \"images\": [], \"title\": None, \"folder\": None, \"error\": None}\n",
        "            try:\n",
        "                soup = fetch_soup(url)\n",
        "                title = extract_title(soup)\n",
        "                item[\"title\"] = title\n",
        "                folder_name = f\"{idx:02d}_{slugify(title)}\"\n",
        "                item[\"folder\"] = folder_name\n",
        "                folder_path = ensure_dir(os.path.join(root_dir, folder_name))\n",
        "\n",
        "                description = extract_description(soup)\n",
        "                with open(os.path.join(folder_path, \"testo.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(description or \"\")\n",
        "\n",
        "                img_urls = extract_gallery_images(soup, url)\n",
        "                for j, img_url in enumerate(img_urls, 1):\n",
        "                    ext = os.path.splitext(urlparse(img_url).path)[1] or \".jpg\"\n",
        "                    ext = ext[:5]\n",
        "                    img_name = f\"img_{j:03d}{ext}\"\n",
        "                    img_dest = os.path.join(folder_path, img_name)\n",
        "                    ok = download_file(img_url, img_dest)\n",
        "                    if ok:\n",
        "                        item[\"images\"].append({\"url\": img_url, \"file\": img_name})\n",
        "                time.sleep(DELAY_SEC)\n",
        "            except Exception as e:\n",
        "                item[\"error\"] = str(e)\n",
        "                print(\"  -> ERRORE:\", e)\n",
        "            manifest.append(item)\n",
        "\n",
        "        with open(os.path.join(root_dir, \"manifest.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        zip_path = shutil.make_archive(root_dir, \"zip\", root_dir)\n",
        "        print(\"\\nPronto!\")\n",
        "        print(\"Cartella:\", root_dir)\n",
        "        print(\"ZIP:\", zip_path)\n",
        "        _show_download_controls(zip_path)\n",
        "\n",
        "btn_start.on_click(run_scraper)\n",
        "\n",
        "display(widgets.VBox([n_items_widget, btn_start, out, download_area]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
